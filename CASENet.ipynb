{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks\n",
    "\n",
    "* Data normalization\n",
    "    * Mobilenet expects data from -1 to 1\n",
    "        * Normalize Input Data or Include in Model\n",
    "        * TFLite Conversion must fit according to decision\n",
    "    * Ground Truth Data: for better inspection Data multiplied by 80. Undo the change in the Data Input Pipeline\n",
    "* Overview in Tutorials:\n",
    "    * tf.function\n",
    "    * Repeat addapted Version of using Build in methods for training, ...\n",
    "    * Save models using keras\n",
    "        * CaseNet first real model: check in implementation of Frey if a Layer needs to be written\n",
    "        * other Example: depth seperable dilated convolution,\n",
    "* Idea\n",
    "    * Loss\n",
    "        * Focal Loss: for imbalanced Data\n",
    "        * In general Loss: just now weight in each dependent on number of Edge Pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "import DataProcessing.data_processing as data_processing\n",
    "import Nets.backbones as backbones\n",
    "import Nets.features as features\n",
    "import Nets.losses as losses\n",
    "import Nets.metrics as metrics\n",
    "import Nets.visualize as visualize\n",
    "import Nets.tools as tools\n",
    "\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model', type=str, required=False, default=None)\n",
    "parser.add_argument('--data', type=str, required=False, default=None)\n",
    "\n",
    "parser.add_argument('--bs', type=int, required=False, default=None)\n",
    "parser.add_argument('--idx', type=int, required=False, default=None)\n",
    "parser.add_argument('--epoch', type=int, required=False, default=None)\n",
    "parser.add_argument('--noise', type=float, required=False, default=None)\n",
    "\n",
    "parser.add_argument('--train_model', action='store_true', default=False)\n",
    "parser.add_argument('--cache', action='store_true', default=False)\n",
    "parser.add_argument('--save', action='store_true', default=False)\n",
    "parser.add_argument('--sigmoid', action='store_true', default=False)\n",
    "parser.add_argument('--focal', action='store_true', default=False)\n",
    "\n",
    "parser.add_argument('--beta_upper', type=float, required=False, default=None)\n",
    "parser.add_argument('--gamma', type=float, required=False, default=None)\n",
    "parser.add_argument('--alpha', type=float, required=False, default=None)\n",
    "\n",
    "file_name = None\n",
    "try:\n",
    "    file_name = __file__\n",
    "except:\n",
    "    print(\"Jupyter Notebook\")\n",
    "       \n",
    "if file_name is None:\n",
    "    args = parser.parse_args(\"\")\n",
    "    args.train_model = True\n",
    "    args.cache = True\n",
    "    #args.save = True\n",
    "    args.save = False\n",
    "    args.sigmoid = False\n",
    "    args.focal = True\n",
    "else:    \n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generall Parameters\n",
    "MODEL= 'CASENET_FOCAL_LOSS_0.7_g2_a2_random' if args.model is None else args.model\n",
    "DATA= 'SceneNetFloorTiledTextureRandom' if args.data is None else args.data\n",
    "#DATA = 'RealRed'\n",
    "TRAIN_DS ='Train'\n",
    "TEST_DS = 'Test'\n",
    "HALF = True\n",
    "\n",
    "# Dataset Loading Parameters\n",
    "# IMG_SIZE_HEIGHT = 1280\n",
    "# IMG_SIZE_WIDTH = 720\n",
    "IMG_SIZE = (1280, 720)\n",
    "IMG_SIZE_MODEL = (1280, 720)\n",
    "NUM_CLASSES = 3\n",
    "MAX_IMG_TRAIN = 500\n",
    "MAX_IMG_TEST = 300\n",
    "SEED = None\n",
    "BATCH_SIZE = 3 if args.bs is None else args.bs\n",
    "CACHE = args.cache\n",
    "NOISE_STD = 0.0 if args.noise is None else args.noise\n",
    "\n",
    "# Model Parameters\n",
    "BACKBONE = \"RESNet50\"\n",
    "BACKBONE_OUTPUT = [0,1,2,4]\n",
    "BACKBONE_WEIGHTS = \"imagenet\"\n",
    "ALPHA = 1\n",
    "FINE_TUNING = False\n",
    "FINE_TUNE_EPOCHS = 10\n",
    "TRAINABLE_IDX = 0 if args.idx is None else args.idx # (3-1), as indexing starts at 0\n",
    "EPOCHS = 80 if args.epoch is None else args.epoch\n",
    "SAVE = args.save\n",
    "TRAIN_MODEL = args.train_model\n",
    "\n",
    "#Model Callback\n",
    "MODEL_SAVE_EPOCH_FREQ = 5\n",
    "DEL_OLD_CHECKPOINTS = False\n",
    "TENSORBOARD = False\n",
    "DEL_OLD_TENSORBOARD = True\n",
    "\n",
    "# LOSS\n",
    "weighted_multi_label_sigmoid_edge_loss = args.sigmoid\n",
    "focal_loss = args.focal\n",
    "\n",
    "beta_upper = 0.7 if args.beta_upper is None else args.beta_upper\n",
    "beta_lower = 1.0 - beta_upper\n",
    "gamma=2.0 if args.gamma is None else args.gamma \n",
    "alpha=2.0 if args.alpha is None else args.alpha\n",
    "class_weighted = True\n",
    "weighted_beta=True\n",
    "\n",
    "\n",
    "# All Pixels have been labeled correctly and thus we don't need to account shifted labels \n",
    "# and a protection band around the labels for the calculation of the metrics\n",
    "\n",
    "# In the work of Frey he mentioned that state of the Art ? is 2% of diagonal. \n",
    "# He takes 1%, I sugest to take a threshold of 3 Pixels. \n",
    "#I don't think that I made more then 3 Pixel mistake in labeling and tracking. Thus this is 0.4%\n",
    "THRESHOLD_EDGE_WIDTH_REAL = 2\n",
    "\n",
    "# Data Augmentation:\n",
    "aug_param = {\"contrast_factor\": 0.8, \"brightness\": 0.2, \"hue\": 0.05, \"saturation\": 0.8, \"gaussian_value\": 0.015,\n",
    "            \"value\": 0.1, \"strength_spot\": 0.5, \"blur\": False, \"sigma\": 1.0}\n",
    "\n",
    "#TESTING\n",
    "test = False\n",
    "if test:\n",
    "    EPOCHS = 10\n",
    "    MAX_IMG_TRAIN = 18\n",
    "    MAX_IMG_TEST = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset, Preprocess Images and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 14:23:51.397148: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-07-11 14:23:51.397171: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (david-ThinkPad-X1-Yoga-Gen-6): /proc/driver/nvidia/version does not exist\n",
      "2022-07-11 14:23:51.398061: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TRAIN Dataset contains 500 images.\n",
      "The TEST Dataset contains 300 images.\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "\n",
    "paths, files = data_processing.path_definitions(HALF, MODEL, DATA, TRAIN_DS, TEST_DS, make_dirs=True)\n",
    "\n",
    "data_processing.clean_model_directories(paths, DEL_OLD_CHECKPOINTS, DEL_OLD_TENSORBOARD)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    \n",
    "    rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "\n",
    "\n",
    "    train_ds, img_count_train = data_processing.load_dataset(paths,\"TRAIN\", IMG_SIZE, IMG_SIZE_MODEL, NUM_CLASSES, HALF, \n",
    "                                                             MAX_IMG_TRAIN, noise_std=NOISE_STD)\n",
    "    train_ds = data_processing.dataset_processing(train_ds, cache=CACHE, shuffle=True, batch_size=BATCH_SIZE, prefetch=True, \n",
    "                                                  img_count=img_count_train, augment=True, rng=rng, aug_param=aug_param)\n",
    "\n",
    "test_ds, img_count_test = data_processing.load_dataset(paths,\"TEST\", IMG_SIZE, IMG_SIZE_MODEL, NUM_CLASSES, HALF, \n",
    "                                                       MAX_IMG_TEST, noise_std=None)\n",
    "test_ds = data_processing.dataset_processing(test_ds, cache=CACHE, shuffle=False, batch_size=BATCH_SIZE, prefetch=True, \n",
    "                                             img_count=img_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TEST Dataset contains 25 images.\n"
     ]
    }
   ],
   "source": [
    "DATA_REAL = 'RealRed'\n",
    "TRAIN_REAL = 'Train'\n",
    "TEST_REAL = 'Test'\n",
    "TEST_HARD_REAL = 'Test Hard'\n",
    "IMG_ONLY_REAL = 'Img Only'\n",
    "BS_REAL = 8\n",
    "\n",
    "paths_real, files_real = data_processing.path_definitions(HALF, MODEL, DATA_REAL, TRAIN_REAL, TEST_REAL, TEST_HARD_REAL, IMG_ONLY_REAL, make_dirs=False)\n",
    "\n",
    "test_real_ds, img_count_test_real = data_processing.load_dataset(paths_real,\"TEST\", IMG_SIZE, IMG_SIZE_MODEL, NUM_CLASSES, HALF, MAX_IMG_TEST)\n",
    "test_real_ds = data_processing.dataset_processing(test_real_ds, cache=False, shuffle=False, batch_size=BS_REAL, prefetch=False, img_count = img_count_test_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weighted_multi_label_sigmoid_edge_loss:\n",
    "    loss = lambda y_true, y_pred : losses.weighted_multi_label_sigmoid_loss(y_true,y_pred,beta_lower=beta_lower,beta_upper=beta_upper, class_weighted=class_weighted)\n",
    "elif focal_loss:\n",
    "    loss = lambda y_true, y_pred : losses.focal_loss_edges(y_true, y_pred, gamma=gamma, alpha=alpha, weighted_beta=weighted_beta,beta_lower=beta_lower,beta_upper=beta_upper, class_weighted=class_weighted)\n",
    "else:\n",
    "    raise ValueError(\"either FocalLoss or WeightedMultiLabelSigmoidLoss must be True\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 640, 360, 12), dtype=tf.float32, name=None), name='concatenate_2/concat:0', description=\"created by layer 'concatenate_2'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.__operators__.getitem_6/strided_slice:0', description=\"created by layer 'tf.__operators__.getitem_6'\")\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 1280, 720,   0           []                               \n",
      "                                3)]                                                               \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_5 (Sl  (None, 1280, 720, 3  0          ['input_4[0][0]']                \n",
      " icingOpLambda)                 )                                                                 \n",
      "                                                                                                  \n",
      " tf.nn.bias_add_1 (TFOpLambda)  (None, 1280, 720, 3  0           ['tf.__operators__.getitem_5[0][0\n",
      "                                )                                ]']                              \n",
      "                                                                                                  \n",
      " base_model (Functional)        [(None, 640, 360, 6  23576448    ['tf.nn.bias_add_1[0][0]']       \n",
      "                                4),                                                               \n",
      "                                 (None, 320, 180, 2                                               \n",
      "                                56),                                                              \n",
      "                                 (None, 160, 90, 51                                               \n",
      "                                2),                                                               \n",
      "                                 (None, 80, 45, 204                                               \n",
      "                                8)]                                                               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 320, 180, 1)  257         ['base_model[0][1]']             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 160, 90, 1)   513         ['base_model[0][2]']             \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 640, 360, 1)  65          ['base_model[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2DTran  (None, 640, 360, 1)  16         ['conv2d_6[0][0]']               \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2DTran  (None, 640, 360, 1)  64         ['conv2d_7[0][0]']               \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 80, 45, 3)    6147        ['base_model[0][3]']             \n",
      "                                                                                                  \n",
      " tf.repeat_3 (TFOpLambda)       (None, 640, 360, 3)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " tf.repeat_4 (TFOpLambda)       (None, 640, 360, 3)  0           ['conv2d_transpose_2[0][0]']     \n",
      "                                                                                                  \n",
      " tf.repeat_5 (TFOpLambda)       (None, 640, 360, 3)  0           ['conv2d_transpose_3[0][0]']     \n",
      "                                                                                                  \n",
      " side5 (Conv2DTranspose)        (None, 640, 360, 3)  2304        ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 640, 360, 12  0           ['tf.repeat_3[0][0]',            \n",
      "                                )                                 'tf.repeat_4[0][0]',            \n",
      "                                                                  'tf.repeat_5[0][0]',            \n",
      "                                                                  'side5[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_1 (TFOpLamb  (4,)                0           ['concatenate_3[0][0]']          \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.cast_1 (TFOpLambda)         (4,)                 0           ['tf.compat.v1.shape_1[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_9 (Sl  ()                  0           ['tf.cast_1[0][0]']              \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_7 (Sl  ()                  0           ['tf.cast_1[0][0]']              \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_8 (Sl  ()                  0           ['tf.cast_1[0][0]']              \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.math.truediv_1 (TFOpLambda)  ()                  0           ['tf.__operators__.getitem_9[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)      (None, None, None,   0           ['concatenate_3[0][0]',          \n",
      "                                3, None)                          'tf.__operators__.getitem_7[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.__operators__.getitem_8[0][0\n",
      "                                                                 ]',                              \n",
      "                                                                  'tf.math.truediv_1[0][0]']      \n",
      "                                                                                                  \n",
      " tf.compat.v1.transpose_1 (TFOp  (None, None, None,   0          ['tf.reshape_1[0][0]']           \n",
      " Lambda)                        None, 3)                                                          \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, None, None,   4           ['tf.compat.v1.transpose_1[0][0]'\n",
      "                                None, 1)                         ]                                \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_1 (TFOpLa  (None, None, None,   0          ['conv2d_9[0][0]']               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mbda)                          None)                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,585,818\n",
      "Trainable params: 23,523,098\n",
      "Non-trainable params: 62,720\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    backbone, output_names = backbones.get_backbone(name=BACKBONE,weights=BACKBONE_WEIGHTS,\n",
    "                                              height=IMG_SIZE[0],width=IMG_SIZE[1],\n",
    "                                              alpha=ALPHA, output_layer = BACKBONE_OUTPUT,\n",
    "                                                            trainable_idx = TRAINABLE_IDX)\n",
    "\n",
    "    upsample_side_1 = tf.keras.layers.Conv2D(1, kernel_size=1, strides=(1, 1), padding='same')(backbone.output[0])\n",
    "    upsample_side_2 = features.side_feature_casenet(backbone.output[1],channels=1,kernel_size_transpose=4,stride_transpose=2)\n",
    "    upsample_side_3 = features.side_feature_casenet(backbone.output[2],channels=1,kernel_size_transpose=8,stride_transpose=4)\n",
    "    #upsample_side_5 = tf.image.resize(backbone.output[3],(int(IMG_SIZE_HEIGHT/16),int(IMG_SIZE_WIDTH/16)))\n",
    "    upsample_side_5 = features.side_feature_casenet(backbone.output[3],channels=NUM_CLASSES,kernel_size_transpose=16,stride_transpose=8,name='side5')\n",
    "\n",
    "    side_outputs = [upsample_side_1,upsample_side_2,upsample_side_3,upsample_side_5]\n",
    "    concat = features.shared_concatenation(side_outputs,NUM_CLASSES)\n",
    "\n",
    "    output = features.fused_classification(concat,NUM_CLASSES,name=\"output\")\n",
    "\n",
    "    model = tf.keras.Model(inputs = backbone.input, outputs = [output,upsample_side_5])\n",
    "    \n",
    "    model.summary()\n",
    "\n",
    "    # tf.keras.utils.plot_model(model,show_shapes = True,to_file = 'h.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORBOARD:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir /home/david/SemesterProject/Models/CASENet/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/david/SemesterProject/tf/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_6328/907785830.py\", line 4, in None  *\n        lambda y_true, y_pred : losses.focal_loss_edges(y_true, y_pred, gamma=gamma, alpha=alpha, weighted_beta=weighted_beta,beta_lower=beta_lower,beta_upper=beta_upper, class_weighted=class_weighted)\n    File \"/home/david/SemesterProject/SGED/Nets/losses.py\", line 72, in focal_loss_edges  *\n        range_reshape = tf.reshape(tf.range(1, y_prediction.shape[-1] + 1), [1, 1, 1, y_prediction.shape[-1]])\n\n    TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# compile model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr_schedule),\n\u001b[1;32m     17\u001b[0m               loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     18\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: [metrics\u001b[38;5;241m.\u001b[39mBinaryAccuracyEdges(threshold_prediction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     19\u001b[0m                                   metrics\u001b[38;5;241m.\u001b[39mF1Edges(threshold_prediction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, threshold_edge_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)]})\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_real_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/david/SemesterProject/tf/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/tmp/ipykernel_6328/907785830.py\", line 4, in None  *\n        lambda y_true, y_pred : losses.focal_loss_edges(y_true, y_pred, gamma=gamma, alpha=alpha, weighted_beta=weighted_beta,beta_lower=beta_lower,beta_upper=beta_upper, class_weighted=class_weighted)\n    File \"/home/david/SemesterProject/SGED/Nets/losses.py\", line 72, in focal_loss_edges  *\n        range_reshape = tf.reshape(tf.range(1, y_prediction.shape[-1] + 1), [1, 1, 1, y_prediction.shape[-1]])\n\n    TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # learning rate schedule\n",
    "    base_learning_rate = 0.0005\n",
    "    end_learning_rate = 0.0001\n",
    "    decay_step = np.ceil(img_count_train / BATCH_SIZE)*EPOCHS\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(base_learning_rate,decay_steps = decay_step,end_learning_rate = end_learning_rate, power = 0.9)\n",
    "\n",
    "    frequency = int(np.ceil(img_count_train / BATCH_SIZE)*MODEL_SAVE_EPOCH_FREQ)+1\n",
    "\n",
    "    logdir = os.path.join(paths['TBLOGS'], datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath = paths[\"CKPT\"]+ \"/ckpt-loss={val_loss:.2f}-epoch={epoch:.2f}-f1={val_f1:.4f}\",\n",
    "                                                    save_weights_only=False,save_best_only=False,monitor=\"val_f1\",verbose=1,save_freq= 'epoch', period=5),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir=logdir,histogram_freq=1)]\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss=loss,\n",
    "                  metrics={'output': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                                      metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\n",
    "\n",
    "    history = model.fit(train_ds, epochs=EPOCHS, validation_data=test_real_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = os.listdir(paths['CKPT'])\n",
    "\n",
    "f1_max = 0\n",
    "for ckpt_name in model_ckpt:\n",
    "    if float(ckpt_name[-4:]) > f1_max:\n",
    "        f1_max = float(ckpt_name[-4:])\n",
    "        model_path = paths['CKPT']+\"/\"+ckpt_name\n",
    "        \n",
    "        print(model_path)\n",
    "\n",
    "custom_objects = {\"BinaryAccuracyEdges\": metrics.BinaryAccuracyEdges,\n",
    "                  \"F1Edges\": metrics.F1Edges,\n",
    "                  \"<lambda>\":loss}\n",
    "\n",
    "model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    plot_losses = [\"loss\", \"output_loss\"]\n",
    "    plot_metrics = [\"output_accuracy_edges\", \"f1\", \"recall\", \"precision\"]\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"training.svg\")\n",
    "\n",
    "    visualize.plot_training_results(res=history.history, losses=plot_losses, metrics=plot_metrics, save=SAVE, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Maximum F1 Score:\n",
    "if not TRAIN_MODEL:\n",
    "    step_width = 0.05\n",
    "    threshold_range = [0.05,0.95]\n",
    "    threshold_array = np.arange(threshold_range[0],threshold_range[1]+step_width,step_width)\n",
    "    threshold_array = np.array([0.025, 0.1, 0.2,0.3,0.4,0.45,0.5,0.55,0.6,0.7,0.8, 0.9, 0.975])\n",
    "\n",
    "    path_metrics_evaluation_plot = os.path.join(paths[\"FIGURES\"],\"threshold_metrics_evaluation_test_ds.svg\")\n",
    "    threshold_f1_max = visualize.plot_threshold_metrics_evaluation_class(model=model, \n",
    "                                                                         ds=test_ds,\n",
    "                                                                         num_classes=NUM_CLASSES,\n",
    "                                                                         threshold_array=threshold_array, \n",
    "                                                                         threshold_edge_width=0, save=SAVE, \n",
    "                                                                         path=path_metrics_evaluation_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    i = 0\n",
    "    for img, label in test_ds.take(4):\n",
    "        img, label = img, label\n",
    "\n",
    "        threshold = 0.5\n",
    "\n",
    "        predictions = model.predict(img)\n",
    "        predictions = tools.predict_class_postprocessing(predictions[0], threshold=threshold)\n",
    "\n",
    "        path = os.path.join(paths[\"FIGURES\"],\"img_test_threshold_{}_{}\".format(threshold,i))\n",
    "        visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=3)\n",
    "\n",
    "        threshold = threshold_f1_max\n",
    "        path = os.path.join(paths[\"FIGURES\"],\"img_test_ods_{}\".format(i))\n",
    "        visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=3)\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FINE_TUNING and TRAIN_MODEL:\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_output = output_names[1-1]\n",
    "\n",
    "    model.trainable = True\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer: \n",
    "    for submodel in model.layers:\n",
    "        if submodel.name == \"base_model\":\n",
    "            for layer in submodel.layers:\n",
    "                layer.trainable = False\n",
    "                if layer.name == fine_tune_output:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    total_epochs =  EPOCHS + FINE_TUNE_EPOCHS\n",
    "\n",
    "    base_learning_rate = 0.00001\n",
    "    end_learning_rate =  0.00001\n",
    "    decay_step = np.floor(img_count_train / BATCH_SIZE)*FINE_TUNE_EPOCHS\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        base_learning_rate,decay_steps = decay_step,end_learning_rate = end_learning_rate, power = 0.9)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss=loss,\n",
    "                  metrics={'output': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                                      metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\n",
    "    \n",
    "\n",
    "    history_fine = model.fit(train_ds, epochs=total_epochs, \n",
    "                               initial_epoch=history.epoch[-1]+1,validation_data=train_ds.take(1), \n",
    "                               callbacks=callbacks)\n",
    "    \n",
    "    plot_losses = [\"loss\", \"output_loss\"]\n",
    "    plot_metrics = [\"output_accuracy_edges\", \"f1\", \"recall\", \"precision\"]\n",
    "    \n",
    "    path = os.path.join(paths[\"FIGURES\"],\"fine_tuning_training.svg\")\n",
    "    \n",
    "    visualize.plot_training_results(res=history.history, res_fine = history_fine.history, \n",
    "                                losses=plot_losses, metrics=plot_metrics, save=SAVE, path=path, epochs=EPOCHS)\n",
    "    \n",
    "    path_metrics_evaluation_plot = os.path.join(paths[\"FIGURES\"],\"threshold_metrics_evaluation__fine_tune_test_ds.svg\")\n",
    "    visualize.plot_threshold_metrics_evaluation(model=model, ds=test_ds, threshold_array=threshold_array, \n",
    "                                        threshold_edge_width=0, save=SAVE, path=path_metrics_evaluation_plot, \n",
    "                                        accuracy_y_lim_min = 0.9)\n",
    "        \n",
    "    for img, label in test_ds.take(1):\n",
    "        img, label = img, label\n",
    "\n",
    "    predictions = model.predict(img)    \n",
    "    predictions = tools.predict_class_postprocessing(predictions[0], threshold = 0.5)\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"fine_tuning_images_0,5\")\n",
    "    visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test DS of Real Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_REAL = 'RealRed'\n",
    "TRAIN_REAL = 'Train'\n",
    "TEST_REAL = 'Test'\n",
    "TEST_HARD_REAL = 'Test Hard'\n",
    "IMG_ONLY_REAL = 'Img Only'\n",
    "BS_REAL = 8\n",
    "\n",
    "paths_real, files_real = data_processing.path_definitions(HALF, MODEL, DATA_REAL, TRAIN_REAL, TEST_REAL, TEST_HARD_REAL, IMG_ONLY_REAL, make_dirs=False)\n",
    "\n",
    "test_real_ds, img_count_test_real = data_processing.load_dataset(paths_real,\"TEST\", IMG_SIZE, IMG_SIZE_MODEL, NUM_CLASSES, HALF, MAX_IMG_TEST)\n",
    "test_real_ds = data_processing.dataset_processing(test_real_ds, cache=False, shuffle=False, batch_size=BS_REAL, prefetch=False, img_count = img_count_test_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    step_width = 0.025\n",
    "    threshold_range = [0.025, 0.975]\n",
    "    threshold_array = np.arange(threshold_range[0],threshold_range[1]+step_width,step_width)\n",
    "\n",
    "    path_metrics_evaluation_plot = os.path.join(paths[\"FIGURES\"],\"threshold_metrics_evaluation_test_real_edge_threshold_{:.1f}.svg\".format(0))\n",
    "    threshold_f1_max = visualize.plot_threshold_metrics_evaluation_class(model=model, ds=test_real_ds, \n",
    "                                                                   num_classes = NUM_CLASSES,\n",
    "                                                                   threshold_array=threshold_array, \n",
    "                                                                   threshold_edge_width=0, save=SAVE, \n",
    "                                                                   path=path_metrics_evaluation_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    for img, label in test_real_ds.take(1):\n",
    "        img, label = img, label\n",
    "\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    predictions = model.predict(img)    \n",
    "    predictions = tools.predict_class_postprocessing(predictions[0], threshold=threshold)\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"images_test_real_threshold_{:.2f}\".format(threshold))\n",
    "    visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=8)\n",
    "\n",
    "    threshold = threshold_f1_max\n",
    "\n",
    "    predictions = model.predict(img)    \n",
    "    predictions = tools.predict_class_postprocessing(predictions[0], threshold=threshold)\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"images_test_real_threshold_ods\")\n",
    "    visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss=loss,\n",
    "                  metrics={'output': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                                      metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\n",
    "\n",
    "if SAVE:\n",
    "    model.save(paths[\"MODEL\"])\n",
    "    \n",
    "    custom_objects = {\"BinaryAccuracyEdges\": metrics.BinaryAccuracyEdges,\n",
    "                      \"F1Edges\": metrics.F1Edges,\n",
    "                      \"<lambda>\":loss}\n",
    "    \n",
    "    model = tf.keras.models.load_model(paths[\"MODEL\"], custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Other, Additional Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addtional Elements to Consider in other Projects\n",
    "\n",
    "* Data augmentation for small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
