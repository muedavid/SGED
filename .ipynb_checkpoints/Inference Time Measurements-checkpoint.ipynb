{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks\n",
    "\n",
    "* Data normalization\n",
    "    * Mobilenet expects data from -1 to 1\n",
    "        * Normalize Input Data or Include in Model\n",
    "        * TFLite Conversion must fit according to decision\n",
    "    * Ground Truth Data: for better inspection Data multiplied by 80. Undo the change in the Data Input Pipeline\n",
    "* Overview in Tutorials:\n",
    "    * tf.function\n",
    "    * Repeat addapted Version of using Build in methods for training, ...\n",
    "    * Save models using keras\n",
    "        * CaseNet first real model: check implementation of Frey if a Layer needs to be written\n",
    "        * other Example: depth seperable dilated convolution,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "#import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "import DataProcessing.data_processing as data_processing\n",
    "import Nets.backbones as backbones\n",
    "import Nets.features as features\n",
    "import Nets.losses as losses\n",
    "import Nets.metrics as metrics\n",
    "import Nets.visualize as visualize\n",
    "import Nets.tools as tools\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model_loaded', type=str, required=False, default=None)\n",
    "parser.add_argument('--data_base_model', type=str, required=False, default=None)\n",
    "\n",
    "parser.add_argument('--sigmoid', action='store_true', default=False)\n",
    "parser.add_argument('--focal', action='store_true', default=False)\n",
    "\n",
    "parser.add_argument('--beta_upper', type=float, required=False, default=None)\n",
    "parser.add_argument('--gamma', type=float, required=False, default=None)\n",
    "parser.add_argument('--alpha', type=float, required=False, default=None)\n",
    "\n",
    "file_name = None\n",
    "try:\n",
    "    file_name = __file__\n",
    "except:\n",
    "    print(\"Jupyter Notebook\")\n",
    "       \n",
    "if file_name is None:\n",
    "    args = parser.parse_args(\"\")\n",
    "    args.train_model = False\n",
    "    args.cache = True\n",
    "    args.save = True\n",
    "    args.sigmoid = False\n",
    "    args.focal = True\n",
    "else:    \n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generall Parameters\n",
    "#MODEL_LOADED = 'CASENET_FOCAL_LOSS_0.5_g2_a2' if args.model_loaded is None else args.model_loaded\n",
    "MODEL_LOADED = 'SGED_FOCAL_LOSS_0.5_g2_a2' if args.model_loaded is None else args.model_loaded\n",
    "DATA_BASE_MODEL_LOADED = 'SceneNetFloorTiledTextureRandom' if args.data_base_model is None else args.data_base_model\n",
    "\n",
    "TRAIN_DS = 'Train'\n",
    "TEST_DS = 'Test'\n",
    "TEST_HARD_DS = 'Test Hard'\n",
    "TEST_IMG_DS = 'Test IMG'\n",
    "HALF = True\n",
    "\n",
    "# Dataset Loading Parameters\n",
    "IMG_SIZE_HEIGHT = 1280\n",
    "IMG_SIZE_WIDTH = 720\n",
    "NUM_CLASSES = 3\n",
    "MAX_IMG_TRAIN = 100\n",
    "MAX_IMG_TEST = 25\n",
    "SEED = None\n",
    "NUM_EVAL = 200\n",
    "NUM_LOOP = 4\n",
    "\n",
    "# Model Parameters\n",
    "SAVE = args.save\n",
    "TRAIN_MODEL = args.train_model\n",
    "\n",
    "\n",
    "# LOSS\n",
    "weighted_multi_label_sigmoid_edge_loss = args.sigmoid\n",
    "focal_loss = args.focal\n",
    "\n",
    "beta_upper = 0.5 if args.beta_upper is None else args.beta_upper\n",
    "beta_lower = 1.0 - beta_upper\n",
    "gamma=2.0 if args.gamma is None else args.gamma \n",
    "alpha=2.0 if args.alpha is None else args.alpha\n",
    "class_weighted = True\n",
    "weighted_beta=True\n",
    "\n",
    "THRESHOLD_EDGE_WIDTH_REAL = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset, Preprocess Images and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 12:01:45.684642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:01:45.696833: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:01:45.697009: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:01:45.697971: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-08 12:01:45.698520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:01:45.698747: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:01:45.698899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:01:46.082414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:01:46.082528: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:01:46.082599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-08 12:01:46.082654: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-08 12:01:46.082679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9750 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:22:00.0, compute capability: 8.6\n",
      "2022-06-08 12:01:46.122350: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2211840000 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "data = tf.random.uniform(\n",
    "    [NUM_EVAL, IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH, 3],\n",
    "    minval=0,\n",
    "    maxval=255,\n",
    "    dtype=tf.dtypes.int32,\n",
    "    seed=None,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "dataset = dataset.batch(1)\n",
    "\n",
    "#dataset = dataset.cache()\n",
    "#dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "paths, files = data_processing.path_definitions(HALF, \"\", \"\", \"\", \"\", \"\", \"\", MODEL_LOADED, DATA_BASE_MODEL_LOADED, make_dirs=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weighted_multi_label_sigmoid_edge_loss:\n",
    "    loss = lambda y_true, y_pred : losses.weighted_multi_label_sigmoid_loss(y_true,y_pred,beta_lower=beta_lower,beta_upper=beta_upper, class_weighted=class_weighted)\n",
    "elif focal_loss:\n",
    "    loss = lambda y_true, y_pred : losses.focal_loss_edges(y_true, y_pred, gamma=gamma, alpha=alpha, weighted_beta=weighted_beta,beta_lower=beta_lower,beta_upper=beta_upper, class_weighted=class_weighted)\n",
    "else:\n",
    "    raise ValueError(\"either FocalLoss or WeightedMultiLabelSigmoidLoss must be True\")\n",
    "    \n",
    "\n",
    "custom_objects = {\"BinaryAccuracyEdges\": metrics.BinaryAccuracyEdges,\n",
    "                  \"F1Edges\": metrics.F1Edges,\n",
    "                  \"<lambda>\":loss}\n",
    "\n",
    "model = tf.keras.models.load_model(paths[\"MODEL LOADED\"], custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate inference time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 12:01:50.845799: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2211840000 exceeds 10% of free system memory.\n",
      "2022-06-08 12:01:52.411232: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n",
      "2022-06-08 12:02:02.226933: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2211840000 exceeds 10% of free system memory.\n",
      "2022-06-08 12:02:11.171659: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2211840000 exceeds 10% of free system memory.\n",
      "2022-06-08 12:02:20.109029: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2211840000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed 35.936 seconds.\n",
      "Time per Image 0.0449\n",
      "Frame per second 22.261718\n"
     ]
    }
   ],
   "source": [
    "# Preheat GPU:\n",
    "i = 0\n",
    "for d in dataset:\n",
    "    model.predict(d)\n",
    "\n",
    "# evaluate:\n",
    "start = time.perf_counter()\n",
    "for i in range(NUM_LOOP):\n",
    "    for d in dataset:\n",
    "        model.predict(d)\n",
    "elapsed = time.perf_counter() - start\n",
    "print('Elapsed %.3f seconds.' % elapsed)\n",
    "print('Time per Image {:.4f}'.format(elapsed/NUM_EVAL/NUM_LOOP))\n",
    "print('Frame per second {:4f}'.format(NUM_EVAL*NUM_LOOP / elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-08 12:07:51.848911: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-06-08 12:07:51.848942: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2022-06-08 12:07:51.849123: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.5_g2_a2\n",
      "2022-06-08 12:07:51.871514: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-06-08 12:07:51.871544: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.5_g2_a2\n",
      "2022-06-08 12:07:51.943605: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-06-08 12:07:52.262727: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.5_g2_a2\n",
      "2022-06-08 12:07:52.412599: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 563477 microseconds.\n",
      "2022-06-08 12:07:53.052881: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1892] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexConv2D\n",
      "Details:\n",
      "\ttf.Conv2D(tensor<?x640x360x12xf32>, tensor<1x1x4x3xf32>) -> (tensor<?x640x360x3xf32>) : {data_format = \"NHWC\", device = \"\", dilations = [1, 1, 1, 1], explicit_paddings = [], padding = \"VALID\", strides = [1, 1, 1, 1], use_cudnn_on_gpu = true}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2022-06-08 12:07:53.053380: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/Conv1/Conv2D because it has fewer than 1024 elements (432).\n",
      "2022-06-08 12:07:53.053388: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/expanded_conv_depthwise_BN/FusedBatchNormV3;model/base_model/expanded_conv_depthwise/depthwise;model/base_model/block_5_project/Conv2D because it has fewer than 1024 elements (144).\n",
      "2022-06-08 12:07:53.053390: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/expanded_conv_project/Conv2D because it has fewer than 1024 elements (128).\n",
      "2022-06-08 12:07:53.053392: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_1_expand/Conv2D because it has fewer than 1024 elements (384).\n",
      "2022-06-08 12:07:53.053394: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_1_depthwise_BN/FusedBatchNormV3;model/base_model/block_1_depthwise/depthwise;model/base_model/block_3_depthwise/depthwise because it has fewer than 1024 elements (432).\n",
      "2022-06-08 12:07:53.053396: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_1_project/Conv2D because it has fewer than 1024 elements (384).\n",
      "2022-06-08 12:07:53.053398: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_2_expand/Conv2D because it has fewer than 1024 elements (384).\n",
      "2022-06-08 12:07:53.053400: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_2_depthwise_BN/FusedBatchNormV3;model/base_model/block_2_depthwise/depthwise;model/base_model/block_3_depthwise/depthwise because it has fewer than 1024 elements (432).\n",
      "2022-06-08 12:07:53.053402: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_2_project/Conv2D because it has fewer than 1024 elements (384).\n",
      "2022-06-08 12:07:53.053404: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_3_expand/Conv2D because it has fewer than 1024 elements (384).\n",
      "2022-06-08 12:07:53.053406: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_3_depthwise_BN/FusedBatchNormV3;model/base_model/block_3_depthwise/depthwise because it has fewer than 1024 elements (432).\n",
      "2022-06-08 12:07:53.053409: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_3_project/Conv2D because it has fewer than 1024 elements (768).\n",
      "2022-06-08 12:07:53.053412: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_4_depthwise_BN/FusedBatchNormV3;model/base_model/block_4_depthwise/depthwise;model/base_model/block_6_depthwise/depthwise because it has fewer than 1024 elements (864).\n",
      "2022-06-08 12:07:53.053414: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_5_depthwise_BN/FusedBatchNormV3;model/base_model/block_5_depthwise/depthwise;model/base_model/block_6_depthwise/depthwise because it has fewer than 1024 elements (864).\n",
      "2022-06-08 12:07:53.053417: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/base_model/block_6_depthwise_BN/FusedBatchNormV3;model/base_model/block_6_depthwise/depthwise because it has fewer than 1024 elements (864).\n",
      "2022-06-08 12:07:53.053421: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_18_dilated_separable_conv/separable_conv2d/depthwise because it has fewer than 1024 elements (288).\n",
      "2022-06-08 12:07:53.053423: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_18_dilated_separable_conv/separable_conv2d because it has fewer than 1024 elements (768).\n",
      "2022-06-08 12:07:53.053425: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_18_conv_separable_conv/separable_conv2d/depthwise because it has fewer than 1024 elements (216).\n",
      "2022-06-08 12:07:53.053427: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_18_conv_separable_conv/separable_conv2d because it has fewer than 1024 elements (576).\n",
      "2022-06-08 12:07:53.053429: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_1_conv/Conv2D because it has fewer than 1024 elements (768).\n",
      "2022-06-08 12:07:53.053431: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_3_dilated_separable_conv/separable_conv2d/depthwise because it has fewer than 1024 elements (288).\n",
      "2022-06-08 12:07:53.053433: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_3_dilated_separable_conv/separable_conv2d because it has fewer than 1024 elements (768).\n",
      "2022-06-08 12:07:53.053435: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_3_conv_separable_conv/separable_conv2d/depthwise because it has fewer than 1024 elements (216).\n",
      "2022-06-08 12:07:53.053437: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_3_conv_separable_conv/separable_conv2d because it has fewer than 1024 elements (576).\n",
      "2022-06-08 12:07:53.053439: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_6_dilated_separable_conv/separable_conv2d/depthwise because it has fewer than 1024 elements (288).\n",
      "2022-06-08 12:07:53.053441: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_6_dilated_separable_conv/separable_conv2d because it has fewer than 1024 elements (768).\n",
      "2022-06-08 12:07:53.053443: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_6_conv_separable_conv/separable_conv2d/depthwise because it has fewer than 1024 elements (216).\n",
      "2022-06-08 12:07:53.053445: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_6_conv_separable_conv/separable_conv2d because it has fewer than 1024 elements (576).\n",
      "2022-06-08 12:07:53.053448: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_9_dilated_separable_conv/separable_conv2d/depthwise1 because it has fewer than 1024 elements (288).\n",
      "2022-06-08 12:07:53.053450: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_9_dilated_separable_conv/separable_conv2d because it has fewer than 1024 elements (768).\n",
      "2022-06-08 12:07:53.053452: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_9_conv_separable_conv/separable_conv2d/depthwise because it has fewer than 1024 elements (216).\n",
      "2022-06-08 12:07:53.053454: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_9_conv_separable_conv/separable_conv2d because it has fewer than 1024 elements (576).\n",
      "2022-06-08 12:07:53.053456: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/daspp_avg_conv/Conv2D because it has fewer than 1024 elements (768).\n",
      "2022-06-08 12:07:53.053458: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/decoder_1_conv/Conv2D because it has fewer than 1024 elements (192).\n",
      "2022-06-08 12:07:53.053460: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/decoder_2_separable_conv/separable_conv2d/depthwise1 because it has fewer than 1024 elements (270).\n",
      "2022-06-08 12:07:53.053462: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/decoder_2_separable_conv/separable_conv2d because it has fewer than 1024 elements (720).\n",
      "2022-06-08 12:07:53.053464: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/decoder_3_separable_conv/separable_conv2d/depthwise because it has fewer than 1024 elements (216).\n",
      "2022-06-08 12:07:53.053466: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/decoder_3_separable_conv/separable_conv2d because it has fewer than 1024 elements (576).\n",
      "2022-06-08 12:07:53.053468: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/decoder_4_conv/Conv2D because it has fewer than 1024 elements (72).\n",
      "2022-06-08 12:07:53.053470: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/side1_conv1x1_conv/Conv2D because it has fewer than 1024 elements (24).\n",
      "2022-06-08 12:07:53.053472: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/side2_conv1x1_conv/Conv2D because it has fewer than 1024 elements (24).\n",
      "2022-06-08 12:07:53.053474: I tensorflow/lite/tools/optimize/quantize_weights.cc:225] Skipping quantization of tensor model/side3_conv1x1_conv/Conv2D because it has fewer than 1024 elements (24).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_LOOP):\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 19\u001b[0m         a \u001b[38;5;241m=\u001b[39m \u001b[43mtflite_quant_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(d)\n\u001b[1;32m     20\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElapsed \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m elapsed)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(paths[\"MODEL LOADED\"])\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_quant_model)\n",
    "\n",
    "# Set model input.\n",
    "input_details = interpreter.get_input_details()\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "start = time.perf_counter()\n",
    "for d in dataset:\n",
    "    interpreter.set_tensor(input_details[0]['index'], d)\n",
    "    interpreter.invoke()\n",
    "    interpreter.tensor(interpreter.get_output_details()[0]['index'])()\n",
    "elapsed = time.perf_counter() - start\n",
    "print('Elapsed %.3f seconds.' % elapsed)\n",
    "print('Time per Image {:.4f}'.format(elapsed/NUM_EVAL/NUM_LOOP))\n",
    "print('Frame per second {:4f}'.format(NUM_EVAL*NUM_LOOP / elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
