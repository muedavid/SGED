{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks\n",
    "\n",
    "* Data normalization\n",
    "    * Mobilenet expects data from -1 to 1\n",
    "        * Normalize Input Data or Include in Model\n",
    "        * TFLite Conversion must fit according to decision\n",
    "    * Ground Truth Data: for better inspection Data multiplied by 80. Undo the change in the Data Input Pipeline\n",
    "* Overview in Tutorials:\n",
    "    * tf.function\n",
    "* Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!for a in /sys/bus/pci/devices/*; do echo 0 | sudo tee -a $a/numa_node; done\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "\n",
    "import DataProcessing.data_processing as data_processing\n",
    "import Nets.backbones as backbones\n",
    "import Nets.features as features\n",
    "import Nets.losses as losses\n",
    "import Nets.metrics as metrics\n",
    "import Nets.visualize as visualize\n",
    "import Nets.tools as tools\n",
    "\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Notebook\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--model', type=str, required=False, default=None)\n",
    "parser.add_argument('--data', type=str, required=False, default=None)\n",
    "\n",
    "parser.add_argument('--bs', type=int, required=False, default=None)\n",
    "parser.add_argument('--idx', type=int, required=False, default=None)\n",
    "parser.add_argument('--epoch', type=int, required=False, default=None)\n",
    "parser.add_argument('--noise', type=float, required=False, default=None)\n",
    "\n",
    "parser.add_argument('--train_model', action='store_true', default=False)\n",
    "parser.add_argument('--cache', action='store_true', default=False)\n",
    "parser.add_argument('--save', action='store_true', default=False)\n",
    "parser.add_argument('--sigmoid', action='store_true', default=False)\n",
    "parser.add_argument('--focal', action='store_true', default=False)\n",
    "\n",
    "parser.add_argument('--beta_upper', type=float, required=False, default=None)\n",
    "parser.add_argument('--gamma', type=float, required=False, default=None)\n",
    "parser.add_argument('--alpha', type=float, required=False, default=None)\n",
    "\n",
    "file_name = None\n",
    "try:\n",
    "    file_name = __file__\n",
    "except:\n",
    "    print(\"Jupyter Notebook\")\n",
    "       \n",
    "if file_name is None:\n",
    "    args = parser.parse_args(\"\")\n",
    "    args.train_model = False\n",
    "    args.cache = True\n",
    "    #args.save = True\n",
    "    args.save = False\n",
    "    args.sigmoid = False\n",
    "    args.focal = True\n",
    "else:    \n",
    "    args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generall Parameters\n",
    "MODEL= 'SGED_FOCAL_LOSS_0.6_g2_a2' if args.model is None else args.model\n",
    "DATA= 'SceneNetFloorTiledTextureRandom' if args.data is None else args.data\n",
    "TRAIN_DS ='Train'\n",
    "TEST_DS = 'Test'\n",
    "HALF = True\n",
    "\n",
    "# Dataset Loading Parameters\n",
    "IMG_SIZE_HEIGHT = 1280\n",
    "IMG_SIZE_WIDTH = 720\n",
    "NUM_CLASSES = 3\n",
    "MAX_IMG_TRAIN = 1500\n",
    "MAX_IMG_TEST = 300\n",
    "SEED = None\n",
    "BATCH_SIZE = 3 if args.bs is None else args.bs\n",
    "CACHE = args.cache\n",
    "NOISE_STD = 0.0 if args.noise is None else args.noise\n",
    "\n",
    "# Model Parameters\n",
    "BACKBONE = \"MobileNetV2\"\n",
    "output = 5\n",
    "BACKBONE_OUTPUT = [1,2,3,output]\n",
    "BACKBONE_WEIGHTS = \"imagenet\"\n",
    "ALPHA = 1\n",
    "FINE_TUNING = False\n",
    "FINE_TUNE_EPOCHS = 10\n",
    "TRAINABLE_IDX = 0 if args.idx is None else args.idx # (3-1), as indexing starts at 0\n",
    "EPOCHS = 80 if args.epoch is None else args.epoch\n",
    "SAVE = args.save\n",
    "TRAIN_MODEL = args.train_model\n",
    "\n",
    "#Model Callback\n",
    "MODEL_SAVE_EPOCH_FREQ = 5\n",
    "DEL_OLD_CHECKPOINTS = False\n",
    "TENSORBOARD = False\n",
    "DEL_OLD_TENSORBOARD = True\n",
    "\n",
    "# LOSS\n",
    "weighted_multi_label_sigmoid_edge_loss = args.sigmoid\n",
    "focal_loss = args.focal\n",
    "\n",
    "beta_upper = 0.6 if args.beta_upper is None else args.beta_upper\n",
    "beta_lower = 1.0 - beta_upper\n",
    "gamma=2.0 if args.gamma is None else args.gamma \n",
    "alpha=2.0 if args.alpha is None else args.alpha\n",
    "class_weighted = True\n",
    "weighted_beta=True\n",
    "\n",
    "\n",
    "# All Pixels have been labeled correctly and thus we don't need to account shifted labels \n",
    "# and a protection band around the labels for the calculation of the metrics\n",
    "\n",
    "# In the work of Frey he mentioned that state of the Art ? is 2% of diagonal. \n",
    "# He takes 1%, I sugest to take a threshold of 3 Pixels. \n",
    "#I don't think that I made more then 3 Pixel mistake in labeling and tracking. Thus this is 0.4%\n",
    "THRESHOLD_EDGE_WIDTH_REAL = 2\n",
    "\n",
    "# Data Augmentation:\n",
    "aug_param = {\"contrast_factor\": 0.9, \"brightness\": 0.1, \"hue\": 0.03, \"saturation\": 0.9, \"gaussian_value\": 0.015,\n",
    "            \"value\": 0.1, \"strength_spot\": 0.3, \"blur\": False, \"sigma\": 1.0}\n",
    "\n",
    "#TESTING\n",
    "test = False\n",
    "if test:\n",
    "    EPOCHS = 10\n",
    "    MAX_IMG_TRAIN = 18\n",
    "    MAX_IMG_TEST = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset, Preprocess Images and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 10:00:33.511168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 10:00:33.523364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 10:00:33.523578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 10:00:33.524308: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-07 10:00:33.524916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 10:00:33.525100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 10:00:33.525229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 10:00:33.893381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 10:00:33.893501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 10:00:33.893576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-07 10:00:33.893630: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-06-07 10:00:33.893655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9784 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:22:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TRAIN Dataset contains 1500 images.\n",
      "The TEST Dataset contains 300 images.\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "\n",
    "paths, files = data_processing.path_definitions(HALF, MODEL, DATA, TRAIN_DS, TEST_DS, make_dirs=True)\n",
    "\n",
    "data_processing.clean_model_directories(paths, DEL_OLD_CHECKPOINTS, DEL_OLD_TENSORBOARD)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    \n",
    "    rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "\n",
    "\n",
    "    train_ds, img_count_train = data_processing.load_dataset(paths,\"TRAIN\", IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH, HALF, \n",
    "                                                             MAX_IMG_TRAIN, noise_std=NOISE_STD)\n",
    "    train_ds = data_processing.dataset_processing(train_ds, cache=CACHE, shuffle=True, batch_size=BATCH_SIZE, prefetch=True, \n",
    "                                                  img_count=img_count_train, augment=True, rng=rng, aug_param=aug_param)\n",
    "\n",
    "test_ds, img_count_test = data_processing.load_dataset(paths,\"TEST\", IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH, HALF, \n",
    "                                                       MAX_IMG_TEST, noise_std=None)\n",
    "test_ds = data_processing.dataset_processing(test_ds, cache=CACHE, shuffle=False, batch_size=BATCH_SIZE, prefetch=True, \n",
    "                                             img_count=img_count_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The TEST Dataset contains 25 images.\n"
     ]
    }
   ],
   "source": [
    "DATA_REAL = 'RealRed'\n",
    "TRAIN_REAL = 'Train'\n",
    "TEST_REAL = 'Test'\n",
    "TEST_HARD_REAL = 'Test Hard'\n",
    "IMG_ONLY_REAL = 'Img Only'\n",
    "BS_REAL = 8\n",
    "\n",
    "paths_real, files_real = data_processing.path_definitions(HALF, MODEL, DATA_REAL, TRAIN_REAL, TEST_REAL, TEST_HARD_REAL, IMG_ONLY_REAL, make_dirs=False)\n",
    "\n",
    "test_real_ds, img_count_test_real = data_processing.load_dataset(paths_real,\"TEST\", IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH, HALF, MAX_IMG_TEST)\n",
    "test_real_ds = data_processing.dataset_processing(test_real_ds, cache=False, shuffle=False, batch_size=BS_REAL, prefetch=False, img_count = img_count_test_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weighted_multi_label_sigmoid_edge_loss:\n",
    "    loss = lambda y_true, y_pred : losses.weighted_multi_label_sigmoid_loss(y_true,y_pred,beta_lower=beta_lower,beta_upper=beta_upper, class_weighted=class_weighted)\n",
    "elif focal_loss:\n",
    "    loss = lambda y_true, y_pred : losses.focal_loss_edges(y_true, y_pred, gamma=gamma, alpha=alpha, weighted_beta=weighted_beta,beta_lower=beta_lower,beta_upper=beta_upper, class_weighted=class_weighted)\n",
    "else:\n",
    "    raise ValueError(\"either FocalLoss or WeightedMultiLabelSigmoidLoss must be True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "\n",
    "    if HALF:\n",
    "        output_dims = (int(IMG_SIZE_HEIGHT/2), int(IMG_SIZE_WIDTH/2))\n",
    "    else:\n",
    "        output_dims = (IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH)\n",
    "\n",
    "    # BACKBONE\n",
    "    backbone, output_names = backbones.get_backbone(name=BACKBONE,weights=BACKBONE_WEIGHTS,\n",
    "                                              height=IMG_SIZE_HEIGHT,width=IMG_SIZE_WIDTH,\n",
    "                                              alpha=ALPHA, output_layer = BACKBONE_OUTPUT, \n",
    "                                              trainable_idx=TRAINABLE_IDX)\n",
    "\n",
    "    # DASPP\n",
    "    daspp = features.DASPP(backbone.output[-1])\n",
    "\n",
    "    # Decoder\n",
    "    decoded = features.decoder(daspp, backbone.output[-1], output_dims=output_dims, NUM_CLASSES=NUM_CLASSES, num_side_filters = 6)\n",
    "\n",
    "\n",
    "    # SIDE FEATURES\n",
    "    # TODO: Upsampling: Nearest NEIGHBOUR ?\n",
    "    upsample_side_1 = features.side_feature_SGED(backbone.output[0], output_dims=output_dims ,interpolation=\"bilinear\", name=\"side1\")\n",
    "    upsample_side_2 = features.side_feature_SGED(backbone.output[1], output_dims=output_dims ,interpolation=\"bilinear\", name=\"side2\")\n",
    "    upsample_side_3 = features.side_feature_SGED(backbone.output[2], output_dims=output_dims ,interpolation=\"bilinear\", name=\"side3\")\n",
    "\n",
    "    # TODO: adaptive weight fusion ?\n",
    "    # CONCATENATE\n",
    "    side_outputs = [upsample_side_1,upsample_side_2,upsample_side_3,decoded]\n",
    "    concat = features.shared_concatenation(side_outputs,NUM_CLASSES)\n",
    "\n",
    "    # FUSE_CLASSIFICATION\n",
    "    output = features.fused_classification(concat,NUM_CLASSES,name=\"output\")\n",
    "\n",
    "    model = tf.keras.Model(inputs = backbone.input, outputs = output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TENSORBOARD:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir /home/david/SemesterProject/Models/CASENet/logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 10:00:48.697875: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:390] Filling up shuffle buffer (this may take a while): 1242 of 1500\n",
      "2022-06-07 10:00:50.886846: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:415] Shuffle buffer filled.\n",
      "2022-06-07 10:00:52.508492: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 263s 485ms/step - loss: 23061.0078 - accuracy_edges: 0.5358 - f1: 0.0180 - precision: 0.0091 - recall: 0.5695 - val_loss: 11340.9961 - val_accuracy_edges: 0.9756 - val_f1: 0.0782 - val_precision: 0.0454 - val_recall: 0.2815\n",
      "Epoch 2/80\n",
      "500/500 [==============================] - 252s 503ms/step - loss: 7756.3169 - accuracy_edges: 0.9838 - f1: 0.2251 - precision: 0.2171 - recall: 0.2531 - val_loss: 2791.7468 - val_accuracy_edges: 0.9961 - val_f1: 0.0482 - val_precision: 0.3457 - val_recall: 0.0259\n",
      "Epoch 3/80\n",
      "500/500 [==============================] - 244s 487ms/step - loss: 2836.0291 - accuracy_edges: 0.9886 - f1: 0.3841 - precision: 0.6208 - recall: 0.2784 - val_loss: 1268.0829 - val_accuracy_edges: 0.9958 - val_f1: 0.1744 - val_precision: 0.2928 - val_recall: 0.1242\n",
      "Epoch 4/80\n",
      "500/500 [==============================] - 237s 473ms/step - loss: 1462.2351 - accuracy_edges: 0.9894 - f1: 0.4760 - precision: 0.6481 - recall: 0.3765 - val_loss: 767.1423 - val_accuracy_edges: 0.9958 - val_f1: 0.2748 - val_precision: 0.3564 - val_recall: 0.2236\n",
      "Epoch 5/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 998.1917 - accuracy_edges: 0.9900 - f1: 0.5398 - precision: 0.6609 - recall: 0.4563\n",
      "Epoch 5: saving model to /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=493.81-epoch=5.00-f1=0.1933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 10:21:33.326669: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=493.81-epoch=5.00-f1=0.1933/assets\n",
      "500/500 [==============================] - 270s 539ms/step - loss: 998.1917 - accuracy_edges: 0.9900 - f1: 0.5398 - precision: 0.6609 - recall: 0.4563 - val_loss: 493.8119 - val_accuracy_edges: 0.9961 - val_f1: 0.1933 - val_precision: 0.4535 - val_recall: 0.1228\n",
      "Epoch 6/80\n",
      "500/500 [==============================] - 245s 489ms/step - loss: 801.2775 - accuracy_edges: 0.9904 - f1: 0.5648 - precision: 0.6575 - recall: 0.4951 - val_loss: 398.9014 - val_accuracy_edges: 0.9961 - val_f1: 0.1679 - val_precision: 0.4116 - val_recall: 0.1055\n",
      "Epoch 7/80\n",
      "500/500 [==============================] - 236s 472ms/step - loss: 637.3973 - accuracy_edges: 0.9914 - f1: 0.6209 - precision: 0.6886 - recall: 0.5654 - val_loss: 338.6400 - val_accuracy_edges: 0.9961 - val_f1: 0.3409 - val_precision: 0.4880 - val_recall: 0.2619\n",
      "Epoch 8/80\n",
      "500/500 [==============================] - 239s 478ms/step - loss: 539.0809 - accuracy_edges: 0.9921 - f1: 0.6870 - precision: 0.7262 - recall: 0.6519 - val_loss: 382.4033 - val_accuracy_edges: 0.9947 - val_f1: 0.3138 - val_precision: 0.3000 - val_recall: 0.3290\n",
      "Epoch 9/80\n",
      "500/500 [==============================] - 237s 472ms/step - loss: 481.9756 - accuracy_edges: 0.9925 - f1: 0.7059 - precision: 0.7338 - recall: 0.6800 - val_loss: 349.7410 - val_accuracy_edges: 0.9951 - val_f1: 0.4136 - val_precision: 0.3688 - val_recall: 0.4706\n",
      "Epoch 10/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 440.7900 - accuracy_edges: 0.9928 - f1: 0.7237 - precision: 0.7395 - recall: 0.7087\n",
      "Epoch 10: saving model to /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=345.68-epoch=10.00-f1=0.3635\n",
      "INFO:tensorflow:Assets written to: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=345.68-epoch=10.00-f1=0.3635/assets\n",
      "500/500 [==============================] - 247s 493ms/step - loss: 440.7900 - accuracy_edges: 0.9928 - f1: 0.7237 - precision: 0.7395 - recall: 0.7087 - val_loss: 345.6838 - val_accuracy_edges: 0.9952 - val_f1: 0.3635 - val_precision: 0.3591 - val_recall: 0.3680\n",
      "Epoch 11/80\n",
      "500/500 [==============================] - 239s 476ms/step - loss: 415.4769 - accuracy_edges: 0.9930 - f1: 0.7370 - precision: 0.7455 - recall: 0.7288 - val_loss: 336.6261 - val_accuracy_edges: 0.9944 - val_f1: 0.4200 - val_precision: 0.3405 - val_recall: 0.5479\n",
      "Epoch 12/80\n",
      "500/500 [==============================] - 232s 463ms/step - loss: 391.0508 - accuracy_edges: 0.9932 - f1: 0.7512 - precision: 0.7494 - recall: 0.7531 - val_loss: 299.8989 - val_accuracy_edges: 0.9948 - val_f1: 0.4406 - val_precision: 0.3699 - val_recall: 0.5445\n",
      "Epoch 13/80\n",
      "500/500 [==============================] - 237s 474ms/step - loss: 370.8988 - accuracy_edges: 0.9933 - f1: 0.7567 - precision: 0.7531 - recall: 0.7605 - val_loss: 338.6552 - val_accuracy_edges: 0.9937 - val_f1: 0.3997 - val_precision: 0.3088 - val_recall: 0.5664\n",
      "Epoch 14/80\n",
      "500/500 [==============================] - 235s 468ms/step - loss: 363.5158 - accuracy_edges: 0.9934 - f1: 0.7593 - precision: 0.7508 - recall: 0.7680 - val_loss: 263.4850 - val_accuracy_edges: 0.9955 - val_f1: 0.4698 - val_precision: 0.4241 - val_recall: 0.5265\n",
      "Epoch 15/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 354.3555 - accuracy_edges: 0.9935 - f1: 0.7666 - precision: 0.7558 - recall: 0.7776\n",
      "Epoch 15: saving model to /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=286.38-epoch=15.00-f1=0.4507\n",
      "INFO:tensorflow:Assets written to: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=286.38-epoch=15.00-f1=0.4507/assets\n",
      "500/500 [==============================] - 244s 487ms/step - loss: 354.3555 - accuracy_edges: 0.9935 - f1: 0.7666 - precision: 0.7558 - recall: 0.7776 - val_loss: 286.3755 - val_accuracy_edges: 0.9954 - val_f1: 0.4507 - val_precision: 0.4119 - val_recall: 0.4975\n",
      "Epoch 16/80\n",
      "500/500 [==============================] - 242s 483ms/step - loss: 339.6255 - accuracy_edges: 0.9937 - f1: 0.7711 - precision: 0.7571 - recall: 0.7857 - val_loss: 295.9648 - val_accuracy_edges: 0.9954 - val_f1: 0.4521 - val_precision: 0.4102 - val_recall: 0.5034\n",
      "Epoch 17/80\n",
      "500/500 [==============================] - 233s 465ms/step - loss: 334.5375 - accuracy_edges: 0.9937 - f1: 0.7736 - precision: 0.7584 - recall: 0.7894 - val_loss: 328.4333 - val_accuracy_edges: 0.9948 - val_f1: 0.4064 - val_precision: 0.3559 - val_recall: 0.4734\n",
      "Epoch 18/80\n",
      "500/500 [==============================] - 239s 477ms/step - loss: 327.2910 - accuracy_edges: 0.9938 - f1: 0.7768 - precision: 0.7605 - recall: 0.7939 - val_loss: 324.3204 - val_accuracy_edges: 0.9939 - val_f1: 0.4087 - val_precision: 0.3194 - val_recall: 0.5671\n",
      "Epoch 19/80\n",
      "500/500 [==============================] - 244s 486ms/step - loss: 322.0721 - accuracy_edges: 0.9938 - f1: 0.7804 - precision: 0.7622 - recall: 0.7994 - val_loss: 347.3553 - val_accuracy_edges: 0.9937 - val_f1: 0.3924 - val_precision: 0.3055 - val_recall: 0.5485\n",
      "Epoch 20/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 317.1129 - accuracy_edges: 0.9939 - f1: 0.7831 - precision: 0.7625 - recall: 0.8048\n",
      "Epoch 20: saving model to /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=360.59-epoch=20.00-f1=0.3653\n",
      "INFO:tensorflow:Assets written to: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=360.59-epoch=20.00-f1=0.3653/assets\n",
      "500/500 [==============================] - 252s 504ms/step - loss: 317.1129 - accuracy_edges: 0.9939 - f1: 0.7831 - precision: 0.7625 - recall: 0.8048 - val_loss: 360.5929 - val_accuracy_edges: 0.9922 - val_f1: 0.3653 - val_precision: 0.2619 - val_recall: 0.6032\n",
      "Epoch 21/80\n",
      "500/500 [==============================] - 247s 493ms/step - loss: 310.0833 - accuracy_edges: 0.9940 - f1: 0.7869 - precision: 0.7649 - recall: 0.8103 - val_loss: 306.8105 - val_accuracy_edges: 0.9947 - val_f1: 0.4435 - val_precision: 0.3644 - val_recall: 0.5663\n",
      "Epoch 22/80\n",
      "500/500 [==============================] - 241s 482ms/step - loss: 308.5427 - accuracy_edges: 0.9940 - f1: 0.7864 - precision: 0.7653 - recall: 0.8088 - val_loss: 267.2101 - val_accuracy_edges: 0.9953 - val_f1: 0.4665 - val_precision: 0.4031 - val_recall: 0.5535\n",
      "Epoch 23/80\n",
      "500/500 [==============================] - 241s 482ms/step - loss: 302.9198 - accuracy_edges: 0.9940 - f1: 0.7881 - precision: 0.7647 - recall: 0.8131 - val_loss: 304.9234 - val_accuracy_edges: 0.9946 - val_f1: 0.4335 - val_precision: 0.3567 - val_recall: 0.5524\n",
      "Epoch 24/80\n",
      "500/500 [==============================] - 252s 503ms/step - loss: 298.3222 - accuracy_edges: 0.9941 - f1: 0.7907 - precision: 0.7668 - recall: 0.8162 - val_loss: 286.8805 - val_accuracy_edges: 0.9958 - val_f1: 0.4772 - val_precision: 0.4502 - val_recall: 0.5077\n",
      "Epoch 25/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 297.0558 - accuracy_edges: 0.9941 - f1: 0.7916 - precision: 0.7663 - recall: 0.8186\n",
      "Epoch 25: saving model to /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=293.76-epoch=25.00-f1=0.4532\n",
      "INFO:tensorflow:Assets written to: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=293.76-epoch=25.00-f1=0.4532/assets\n",
      "500/500 [==============================] - 256s 511ms/step - loss: 297.0558 - accuracy_edges: 0.9941 - f1: 0.7916 - precision: 0.7663 - recall: 0.8186 - val_loss: 293.7588 - val_accuracy_edges: 0.9949 - val_f1: 0.4532 - val_precision: 0.3790 - val_recall: 0.5635\n",
      "Epoch 26/80\n",
      "500/500 [==============================] - 246s 490ms/step - loss: 292.1059 - accuracy_edges: 0.9942 - f1: 0.7957 - precision: 0.7685 - recall: 0.8249 - val_loss: 292.4897 - val_accuracy_edges: 0.9951 - val_f1: 0.4520 - val_precision: 0.3839 - val_recall: 0.5494\n",
      "Epoch 27/80\n",
      "500/500 [==============================] - 239s 477ms/step - loss: 287.9130 - accuracy_edges: 0.9942 - f1: 0.7975 - precision: 0.7702 - recall: 0.8268 - val_loss: 298.7018 - val_accuracy_edges: 0.9951 - val_f1: 0.4517 - val_precision: 0.3901 - val_recall: 0.5363\n",
      "Epoch 28/80\n",
      "500/500 [==============================] - 241s 482ms/step - loss: 286.4593 - accuracy_edges: 0.9942 - f1: 0.7972 - precision: 0.7709 - recall: 0.8253 - val_loss: 300.0858 - val_accuracy_edges: 0.9954 - val_f1: 0.4571 - val_precision: 0.4151 - val_recall: 0.5086\n",
      "Epoch 29/80\n",
      "500/500 [==============================] - 243s 484ms/step - loss: 286.0081 - accuracy_edges: 0.9943 - f1: 0.7999 - precision: 0.7718 - recall: 0.8301 - val_loss: 337.5460 - val_accuracy_edges: 0.9957 - val_f1: 0.4593 - val_precision: 0.4419 - val_recall: 0.4782\n",
      "Epoch 30/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 282.3680 - accuracy_edges: 0.9943 - f1: 0.7971 - precision: 0.7676 - recall: 0.8290\n",
      "Epoch 30: saving model to /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=365.37-epoch=30.00-f1=0.4071\n",
      "INFO:tensorflow:Assets written to: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=365.37-epoch=30.00-f1=0.4071/assets\n",
      "500/500 [==============================] - 256s 512ms/step - loss: 282.3680 - accuracy_edges: 0.9943 - f1: 0.7971 - precision: 0.7676 - recall: 0.8290 - val_loss: 365.3669 - val_accuracy_edges: 0.9943 - val_f1: 0.4071 - val_precision: 0.3345 - val_recall: 0.5200\n",
      "Epoch 31/80\n",
      "500/500 [==============================] - 241s 481ms/step - loss: 276.6854 - accuracy_edges: 0.9943 - f1: 0.8031 - precision: 0.7739 - recall: 0.8345 - val_loss: 303.9171 - val_accuracy_edges: 0.9957 - val_f1: 0.4828 - val_precision: 0.4435 - val_recall: 0.5297\n",
      "Epoch 32/80\n",
      "500/500 [==============================] - 238s 475ms/step - loss: 274.0344 - accuracy_edges: 0.9944 - f1: 0.8037 - precision: 0.7754 - recall: 0.8342 - val_loss: 349.7722 - val_accuracy_edges: 0.9937 - val_f1: 0.4033 - val_precision: 0.3140 - val_recall: 0.5635\n",
      "Epoch 33/80\n",
      "500/500 [==============================] - 240s 479ms/step - loss: 274.8141 - accuracy_edges: 0.9944 - f1: 0.8038 - precision: 0.7731 - recall: 0.8371 - val_loss: 311.4394 - val_accuracy_edges: 0.9948 - val_f1: 0.4361 - val_precision: 0.3643 - val_recall: 0.5430\n",
      "Epoch 34/80\n",
      "500/500 [==============================] - 239s 477ms/step - loss: 274.7563 - accuracy_edges: 0.9944 - f1: 0.8051 - precision: 0.7748 - recall: 0.8379 - val_loss: 504.3009 - val_accuracy_edges: 0.9905 - val_f1: 0.2814 - val_precision: 0.1973 - val_recall: 0.4902\n",
      "Epoch 35/80\n",
      "500/500 [==============================] - ETA: 0s - loss: 272.2271 - accuracy_edges: 0.9944 - f1: 0.8031 - precision: 0.7733 - recall: 0.8352\n",
      "Epoch 35: saving model to /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=371.89-epoch=35.00-f1=0.3803\n",
      "INFO:tensorflow:Assets written to: /home/david/SemesterProject/Models/SceneNetFloorTiledTextureRandom/SGED_FOCAL_LOSS_0.6_g2_a2/CKPT/ckpt-loss=371.89-epoch=35.00-f1=0.3803/assets\n",
      "500/500 [==============================] - 248s 495ms/step - loss: 272.2271 - accuracy_edges: 0.9944 - f1: 0.8031 - precision: 0.7733 - recall: 0.8352 - val_loss: 371.8864 - val_accuracy_edges: 0.9937 - val_f1: 0.3803 - val_precision: 0.3039 - val_recall: 0.5080\n",
      "Epoch 36/80\n",
      " 59/500 [==>...........................] - ETA: 3:49 - loss: 277.1715 - accuracy_edges: 0.9942 - f1: 0.8037 - precision: 0.7744 - recall: 0.8353"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# compile model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlr_schedule),\n\u001b[1;32m     17\u001b[0m               loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     18\u001b[0m               metrics\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m: [metrics\u001b[38;5;241m.\u001b[39mBinaryAccuracyEdges(threshold_prediction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m     19\u001b[0m                                   metrics\u001b[38;5;241m.\u001b[39mF1Edges(threshold_prediction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, threshold_edge_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)]})\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_real_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/keras/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1379\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1380\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1381\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1382\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1383\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1384\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1386\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2956\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2954\u001b[0m   (graph_function,\n\u001b[1;32m   2955\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1854\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/SemesterProject/tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if TRAIN_MODEL:\n",
    "    # learning rate schedule\n",
    "    base_learning_rate = 0.0015\n",
    "    end_learning_rate = 0.0005\n",
    "    decay_step = np.ceil(img_count_train / BATCH_SIZE)*EPOCHS\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(base_learning_rate,decay_steps = decay_step,end_learning_rate = end_learning_rate, power = 0.9)\n",
    "\n",
    "    frequency = int(np.ceil(img_count_train / BATCH_SIZE)*MODEL_SAVE_EPOCH_FREQ)+1\n",
    "\n",
    "    logdir = os.path.join(paths['TBLOGS'], datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath = paths[\"CKPT\"]+ \"/ckpt-loss={val_loss:.2f}-epoch={epoch:.2f}-f1={val_f1:.4f}\",\n",
    "                                                    save_weights_only=False,save_best_only=False,monitor=\"val_f1\",verbose=1,save_freq= 'epoch', period=5),\n",
    "                tf.keras.callbacks.TensorBoard(log_dir=logdir,histogram_freq=1)]\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss=loss,\n",
    "                  metrics={'output': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                                      metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\n",
    "\n",
    "    history = model.fit(train_ds, epochs=EPOCHS, validation_data=test_real_ds, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = os.listdir(paths['CKPT'])\n",
    "\n",
    "f1_max = 0\n",
    "for ckpt_name in model_ckpt:\n",
    "    if float(ckpt_name[-4:]) > f1_max:\n",
    "        f1_max = float(ckpt_name[-4:])\n",
    "        model_path = paths['CKPT']+\"/\"+ckpt_name\n",
    "        \n",
    "        print(model_path)\n",
    "\n",
    "custom_objects = {\"BinaryAccuracyEdges\": metrics.BinaryAccuracyEdges,\n",
    "                  \"F1Edges\": metrics.F1Edges,\n",
    "                  \"<lambda>\":loss}\n",
    "\n",
    "model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    plot_losses = [\"loss\", \"output_loss\"]\n",
    "    plot_metrics = [\"output_accuracy_edges\", \"f1\", \"recall\", \"precision\"]\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"training.svg\")\n",
    "\n",
    "    visualize.plot_training_results(res=history.history, losses=plot_losses, metrics=plot_metrics, save=SAVE, path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Maximum F1 Score:\n",
    "if not TRAIN_MODEL:\n",
    "    step_width = 0.05\n",
    "    threshold_range = [0.05,0.95]\n",
    "    threshold_array = np.arange(threshold_range[0],threshold_range[1]+step_width,step_width)\n",
    "    threshold_array = np.array([0.025, 0.1, 0.2,0.3,0.4,0.45,0.5,0.55,0.6,0.7,0.8, 0.9, 0.975])\n",
    "\n",
    "    path_metrics_evaluation_plot = os.path.join(paths[\"FIGURES\"],\"threshold_metrics_evaluation_test_ds.svg\")\n",
    "    threshold_f1_max = visualize.plot_threshold_metrics_evaluation_class(model=model, \n",
    "                                                                         ds=test_ds,\n",
    "                                                                         num_classes=NUM_CLASSES,\n",
    "                                                                         threshold_array=threshold_array, \n",
    "                                                                         threshold_edge_width=0, save=SAVE, \n",
    "                                                                         path=path_metrics_evaluation_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    i = 0\n",
    "    for img, label in test_ds.take(4):\n",
    "        img, label = img, label\n",
    "\n",
    "        threshold = 0.5\n",
    "\n",
    "        predictions = model.predict(img)\n",
    "        predictions = tools.predict_class_postprocessing(predictions[0], threshold=threshold)\n",
    "\n",
    "        path = os.path.join(paths[\"FIGURES\"],\"img_test_threshold_{}_{}\".format(threshold,i))\n",
    "        visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=3)\n",
    "\n",
    "        threshold = threshold_f1_max\n",
    "        path = os.path.join(paths[\"FIGURES\"],\"img_test_ods_{}\".format(i))\n",
    "        visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=3)\n",
    "\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if FINE_TUNING and TRAIN_MODEL:\n",
    "\n",
    "    # Fine-tune from this layer onwards\n",
    "    fine_tune_output = output_names[1-1]\n",
    "\n",
    "    model.trainable = True\n",
    "\n",
    "    # Freeze all the layers before the `fine_tune_at` layer: \n",
    "    for submodel in model.layers:\n",
    "        if submodel.name == \"base_model\":\n",
    "            for layer in submodel.layers:\n",
    "                layer.trainable = False\n",
    "                if layer.name == fine_tune_output:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    total_epochs =  EPOCHS + FINE_TUNE_EPOCHS\n",
    "\n",
    "    base_learning_rate = 0.00001\n",
    "    end_learning_rate =  0.00001\n",
    "    decay_step = np.floor(img_count_train / BATCH_SIZE)*FINE_TUNE_EPOCHS\n",
    "    lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "        base_learning_rate,decay_steps = decay_step,end_learning_rate = end_learning_rate, power = 0.9)\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss=loss,\n",
    "                  metrics={'output': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                                      metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\n",
    "    \n",
    "\n",
    "    history_fine = model.fit(train_ds, epochs=total_epochs, \n",
    "                               initial_epoch=history.epoch[-1]+1,validation_data=train_ds.take(1), \n",
    "                               callbacks=callbacks)\n",
    "    \n",
    "    plot_losses = [\"loss\", \"output_loss\"]\n",
    "    plot_metrics = [\"output_accuracy_edges\", \"f1\", \"recall\", \"precision\"]\n",
    "    \n",
    "    path = os.path.join(paths[\"FIGURES\"],\"fine_tuning_training.svg\")\n",
    "    \n",
    "    visualize.plot_training_results(res=history.history, res_fine = history_fine.history, \n",
    "                                losses=plot_losses, metrics=plot_metrics, save=SAVE, path=path, epochs=EPOCHS)\n",
    "    \n",
    "    path_metrics_evaluation_plot = os.path.join(paths[\"FIGURES\"],\"threshold_metrics_evaluation__fine_tune_test_ds.svg\")\n",
    "    visualize.plot_threshold_metrics_evaluation(model=model, ds=test_ds, threshold_array=threshold_array, \n",
    "                                        threshold_edge_width=0, save=SAVE, path=path_metrics_evaluation_plot, \n",
    "                                        accuracy_y_lim_min = 0.9)\n",
    "        \n",
    "    for img, label in test_ds.take(1):\n",
    "        img, label = img, label\n",
    "\n",
    "    predictions = model.predict(img)    \n",
    "    predictions = tools.predict_class_postprocessing(predictions[0], threshold = 0.5)\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"fine_tuning_images_0,5\")\n",
    "    visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Test DS of Real Images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DATA_REAL = 'RealRed'\n",
    "TRAIN_REAL = 'Train'\n",
    "TEST_REAL = 'Test'\n",
    "TEST_HARD_REAL = 'Test Hard'\n",
    "IMG_ONLY_REAL = 'Img Only'\n",
    "BS_REAL = 8\n",
    "\n",
    "paths_real, files_real = data_processing.path_definitions(HALF, MODEL, DATA_REAL, TRAIN_REAL, TEST_REAL, TEST_HARD_REAL, IMG_ONLY_REAL, make_dirs=False)\n",
    "\n",
    "test_real_ds, img_count_test_real = data_processing.load_dataset(paths_real,\"TEST\", IMG_SIZE_HEIGHT, IMG_SIZE_WIDTH, HALF, MAX_IMG_TEST)\n",
    "test_real_ds = data_processing.dataset_processing(test_real_ds, cache=False, shuffle=False, batch_size=BS_REAL, prefetch=False, img_count = img_count_test_real)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    step_width = 0.025\n",
    "    threshold_range = [0.025, 0.975]\n",
    "    threshold_array = np.arange(threshold_range[0],threshold_range[1]+step_width,step_width)\n",
    "\n",
    "    path_metrics_evaluation_plot = os.path.join(paths[\"FIGURES\"],\"threshold_metrics_evaluation_test_real_edge_threshold_{:.1f}.svg\".format(0))\n",
    "    threshold_f1_max = visualize.plot_threshold_metrics_evaluation_class(model=model, ds=test_real_ds, \n",
    "                                                                   num_classes = NUM_CLASSES,\n",
    "                                                                   threshold_array=threshold_array, \n",
    "                                                                   threshold_edge_width=0, save=SAVE, \n",
    "                                                                   path=path_metrics_evaluation_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TRAIN_MODEL:\n",
    "    for img, label in test_real_ds.take(1):\n",
    "        img, label = img, label\n",
    "\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    predictions = model.predict(img)    \n",
    "    predictions = tools.predict_class_postprocessing(predictions[0], threshold=threshold)\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"images_test_real_threshold_{:.2f}\".format(threshold))\n",
    "    visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=8)\n",
    "\n",
    "    threshold = threshold_f1_max\n",
    "\n",
    "    predictions = model.predict(img)    \n",
    "    predictions = tools.predict_class_postprocessing(predictions[0], threshold=threshold)\n",
    "\n",
    "    path = os.path.join(paths[\"FIGURES\"],\"images_test_real_threshold_ods\")\n",
    "    visualize.plot_images(images=img, labels=label, predictions=predictions, save=SAVE, path=path, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss=loss,\n",
    "                  metrics={'output': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                                      metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\n",
    "\n",
    "if SAVE:\n",
    "    model.save(paths[\"MODEL\"])\n",
    "    \n",
    "    custom_objects = {\"BinaryAccuracyEdges\": metrics.BinaryAccuracyEdges,\n",
    "                      \"F1Edges\": metrics.F1Edges,\n",
    "                      \"<lambda>\":loss}\n",
    "    \n",
    "    model = tf.keras.models.load_model(paths[\"MODEL\"], custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for img, label in test_ds.take(1):\n",
    "    img, label = img, label\n",
    "\n",
    "    \n",
    "predictions = model.predict(img)\n",
    "predictions = tools.predict_class_postprocessing(predictions, threshold = 0.5)\n",
    "\n",
    "path = os.path.join(paths[\"FIGURES\"],\"fine_tuning_images_0,5\")\n",
    "tools.plot_images(images=img, labels=label, predictions=predictions,SAVE = SAVE, path = path, BS=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  loss=loss,\n",
    "                  metrics={'output': [metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                                      metrics.F1Edges(threshold_prediction=0, threshold_edge_width=0)]})\n",
    "\n",
    "if SAVE:\n",
    "    model.save(paths[\"MODEL\"])\n",
    "    \n",
    "    custom_objects = {\"BinaryAccuracyEdges\": metrics.BinaryAccuracyEdges,\n",
    "                      \"F1Edges\": metrics.F1Edges,\n",
    "                      \"<lambda>\":loss}\n",
    "    \n",
    "    model = tf.keras.models.load_model(paths[\"MODEL\"], custom_objects=custom_objects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
