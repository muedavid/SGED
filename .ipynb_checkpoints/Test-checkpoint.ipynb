{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-07 16:11:23.292466: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-07 16:11:23.666550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:22:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "\n",
    "\n",
    "import DataProcessing.data_processing as data_processing\n",
    "import Nets.backbones as backbones\n",
    "import Nets.features as features\n",
    "import Nets.losses as losses\n",
    "import Nets.metrics as metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "\n",
    "#np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperable Conv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "input = tf.keras.Input(shape=[10,10,3])\n",
    "sep = tf.keras.layers.SeparableConv2D(filters=2,kernel_size=3,use_bias=False)\n",
    "output = sep(input)\n",
    "model = tf.keras.Model(input,output)\n",
    "model.summary()\n",
    "input = tf.keras.Input(shape=[10,10,3])\n",
    "sep = tf.keras.layers.Conv2D(2,3,use_bias = False)\n",
    "output = sep(input)\n",
    "model = tf.keras.Model(input,output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resize Operation "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x = tf.constant([[[[1.0],[1.0]],[[1.0],[1.0]]]])\n",
    "y = tf.constant([[[[4.0],[4.0]],[[4.0],[4.0]]]])\n",
    "\n",
    "d = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "d = d.batch(1)\n",
    "inputs = tf.keras.Input(shape=[2,2,1])\n",
    "x = tf.keras.layers.Conv2D(1,(3,3),(2,2),padding='same',use_bias=False,kernel_initializer = tf.keras.initializers.Constant(value=2.0))(inputs)\n",
    "\n",
    "\n",
    "#z = tf.keras.layers.Resizing(2,2,interpolation='nearest')(x)\n",
    "z = tf.image.resize(x,[2,2])\n",
    "model = tf.keras.Model(inputs,z)\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(0.05),loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "model.fit(d, epochs=10, validation_data=d)\n",
    "\n",
    "print(model.predict(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric and Own Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 9ms/step - loss: 1.3181 - accuracy_edges: 0.5000\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3181 - accuracy_edges: 0.5000 - recall_edges: 0.5000 - precision_edges: 1.0000 - val_loss: 1.3181 - val_accuracy_edges: 0.5000 - val_recall_edges: 0.5000 - val_precision_edges: 1.0000\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# LOSS\n",
    "WeightedMultiLabelSigmoidEdgeLoss = True\n",
    "if WeightedMultiLabelSigmoidEdgeLoss:\n",
    "    Beta_upper = 0.95\n",
    "    Beta_lower = 0.05\n",
    "    classWeighted = False\n",
    "\n",
    "FocalLoss = False\n",
    "if FocalLoss:\n",
    "    gamma=2\n",
    "    alpha=1\n",
    "    weighted_beta=True\n",
    "    Beta_upper=0.95\n",
    "    Beta_lower=0.05\n",
    "    classWeighted=False\n",
    "    \n",
    "\n",
    "if WeightedMultiLabelSigmoidEdgeLoss:\n",
    "    loss = lambda y_true, y_pred : losses.WeightedMultiLabelSigmoidLoss(y_true,y_pred,beta_lower=Beta_lower,beta_upper=Beta_upper, class_weighted=classWeighted)\n",
    "elif FocalLoss:\n",
    "    loss = lambda y_true, y_pred : losses.FocalLossEdges(y_true, y_pred, gamma=gamma, alpha=alpha, weighted_beta=weighted_beta,beta_lower=beta_lower,beta_upper=Beta_upper, class_weighted=classWeighted)\n",
    "else:\n",
    "    raise ValueError(\"either FocalLoss or WeightedMultiLabelSigmoidLoss must be True\")\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "y = tf.constant([[[[1.0],[0.0]],[[0.0],[0.0]]]])\n",
    "pred = tf.constant([[[[1.0, -1.0], [1.0, -1.0]],[[-1.0,-1.0],[-1.0,-1.0]]]])\n",
    "#y = tf.constant([[[[1.0],[0.0]],[[2.0],[0.0]]],[[[1.0],[0.0]],[[2.0],[0.0]]]])\n",
    "#pred = tf.constant([[[[1.0, -1.0], [-1.0, -1.0]],[[-1.0,1.0],[-1.0,-1.0]]],[[[-1.0, -1.0], [-1.0, -1.0]],[[-1.0,1.0],[-1.0,-1.0]]]])\n",
    "\n",
    "d = tf.data.Dataset.from_tensor_slices((pred,y))\n",
    "d = d.batch(BATCH_SIZE)\n",
    "\n",
    "inputs = tf.keras.Input(shape=[2,2,2])\n",
    "model = tf.keras.Model(inputs,inputs)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=loss,\n",
    "              metrics = metrics.BinaryAccuracyEdges(threshold_prediction = 0))\n",
    "\n",
    "history = model.fit(d, epochs=1)\n",
    "\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=loss,\n",
    "              metrics=[metrics.BinaryAccuracyEdges(threshold_prediction=0),\n",
    "                       metrics.RecallEdges(threshold_prediction=0, threshold_edge_width=1),\n",
    "                       metrics.PrecisionEdges(threshold_prediction=0, threshold_edge_width=1)])\n",
    "\n",
    "\n",
    "history = model.fit(d, epochs=1, validation_data=d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Modell to test things like leraning rates"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/scalars/"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train = np.arange(1,50,0.2)\n",
    "y_train = np.power(x_train,2)+np.random.uniform(-4,4,(x_train.shape))\n",
    "x_val = x_train[-10:]\n",
    "y_val = y_train[-10:]\n",
    "\n",
    "\n",
    "inputs = tf.keras.Input(shape=[1])\n",
    "#x = tf.keras.layers.Dense(3,activation = 'relu')(inputs)\n",
    "x = tf.keras.layers.Dense(5,activation = 'relu')(inputs)\n",
    "x = tf.keras.layers.Dense(5,activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(5,activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(5,activation = 'relu')(x)\n",
    "x = tf.keras.layers.Dense(5,activation = 'relu')(x)\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs,output)\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 100\n",
    "decay_step = np.floor(x_train.shape[0]/ BATCH_SIZE)*EPOCHS\n",
    "lr = tf.keras.optimizers.schedules.PolynomialDecay(0.005,decay_steps = decay_step,end_learning_rate = 0.001, power = 1)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "logdir_base = \"logs/scalars/\"\n",
    "!rm -rf logs\n",
    "logdir = logdir_base + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir=logdir,histogram_freq=1)]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(x_val, y_val),\n",
    "    callbacks = callbacks\n",
    ")\n",
    "\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "#plt.subplot(2, 1, 1)\n",
    "#plt.plot(acc, label='Training Accuracy')\n",
    "#plt.plot(val_acc, label='Validation Accuracy')\n",
    "#plt.legend(loc='lower right')\n",
    "#plt.ylabel('Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "#plt.title('Training and Validation Accuracy')\n",
    "\n",
    "#plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,100])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(x_train,y_train)\n",
    "plt.plot(x_train,model.predict(x_train))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "model.save(\"test\")\n",
    "\n",
    "custom_objects = {\"BinaryAccuracy_Edges\": metrics.BinaryAccuracy_Edges, \"WeightedMultiLabelSigmoidLoss\": losses.WeightedMultiLabelSigmoidLoss}\n",
    "saved_model = tf.keras.models.load_model(\"test\", custom_objects = custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N_true_positive with widen edges."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_true = tf.constant([[[[1.0],[0.0],[0.0]],[[2.0],[0.0],[0.0]],[[0.0],[0.0],[0.0]]]])\n",
    "\n",
    "pred = tf.constant([[[[1.0,0.0],[0.0,0.0],[0.0,0.0]],[[0.0,1.0],[0.0,0.0],[0.0,0.0]],[[1.0,0.0],[0.0,0.0],[0.0,0.0]]]])\n",
    "\n",
    "print(pred[:,:,:,0])\n",
    "print(pred[:,:,:,1])\n",
    "\n",
    "pred = tf.cast(pred,dtype=tf.float32)\n",
    "y_true = tf.cast(y_true, dtype=tf.int32)\n",
    "Range = tf.range(1, 2 + 1)\n",
    "rangeReshape = tf.reshape(Range, [1, 1, 1, 2])\n",
    "y_true = tf.cast(rangeReshape == y_true, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filters = tf.ones([3,3,2,1], tf.float32)\n",
    "\n",
    "widen = tf.nn.depthwise_conv2d(\n",
    "    y_true,\n",
    "    filters,\n",
    "    strides=[1,1,1,1],\n",
    "    padding=\"SAME\",\n",
    "    data_format=None,\n",
    "    dilations=None,\n",
    "    name=None\n",
    ")\n",
    "\n",
    "widen = tf.clip_by_value(widen,0,1)\n",
    "\n",
    "print(widen[:,:,:,0])\n",
    "\n",
    "N_tp = tf.cast(widen == pred,tf.float32)*widen\n",
    "\n",
    "print(N_tp[:,:,:,0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MF Metric"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#tf.config.run_functions_eagerly(True)      \n",
    "\n",
    "# y = tf.constant([[[[1.0],[0.0]],[[2.0],[0.0]]]])\n",
    "# pred = tf.constant([[[[-1.0, -1.0], [1.0, -1.0]],[[-1.0,1.0],[-1.0,-1.0]]]])\n",
    "y = tf.constant([[[[1.0],[1.0]],[[2.0],[0.0]]],[[[1.0],[0.0]],[[2.0],[0.0]]]])\n",
    "pred = tf.constant([[[[1.0, 1.0], [1.0, -1.0]],[[-1.0,1.0],[-1.0,-1.0]]],[[[-1.0, -1.0], [-1.0, -1.0]],[[-1.0,1.0],[-1.0,-1.0]]]])\n",
    "\n",
    "d = tf.data.Dataset.from_tensor_slices((pred,y))\n",
    "\n",
    "d = d.batch(tf.data.experimental.cardinality(d).numpy())\n",
    "#d = d.batch(1)\n",
    "for y_pred,y_true in d.take(1):\n",
    "    y_pred, y_true = y_pred.numpy(), y_true.numpy()\n",
    "    \n",
    "F1, pre, rec = metrics.F1_WidenEdges(y_pred,y_true,0,1)\n",
    "print(F1)\n",
    "\n",
    "\n",
    "m = metrics.F1_Edges(threshold = tf.constant(0), widen = tf.constant(3))\n",
    "m.update_state(y_true = y_true, y_pred = y_pred)\n",
    "F,pre,rec = m.result()\n",
    "print(F)\n",
    "\n",
    "\n",
    "\n",
    "metric = tfa.metrics.F1Score(num_classes=2, threshold=0.5)\n",
    "\n",
    "y_pred = tf.nn.sigmoid(y_pred)\n",
    "\n",
    "y_true = tf.cast(y_true, dtype=tf.int32)\n",
    "Range = tf.range(1, y_pred.shape[-1] + 1)\n",
    "rangeReshape = tf.reshape(Range, [1, 1, 1, y_pred.shape[-1]])\n",
    "y_true = tf.cast(rangeReshape == y_true, dtype=tf.int32)\n",
    "\n",
    "metric.update_state(tf.reshape(y_true,[-1,2]), tf.reshape(y_pred, [-1,2]))\n",
    "result = metric.result()\n",
    "print(result.numpy())\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "d = tf.data.Dataset.from_tensor_slices((pred,y))\n",
    "\n",
    "#d = d.batch(tf.data.experimental.cardinality(d).numpy())\n",
    "\n",
    "m = metrics.F1_Edges(threshold = tf.constant(0), widen = tf.constant(1))\n",
    "\n",
    "d = d.batch(1)\n",
    "for y_pred,y_true in d:\n",
    "    y_pred, y_true = y_pred, y_true\n",
    "    \n",
    "    m.update_state(y_true = y_true, y_pred = y_pred)\n",
    "    \n",
    "F,pre,rec = m.result()\n",
    "\n",
    "print(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
