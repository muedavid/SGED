{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links and Information Usefull:\n",
    "\n",
    "* Documentation about TFRecords: https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "* Scripts to transform Dataset with images to TFRecords: https://keras.io/examples/keras_recipes/creating_tfrecords/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'SceneNet'\n",
    "MODEL = 'Real'\n",
    "OUTPUT = 'output'\n",
    "half = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if half:\n",
    "    paths = {\n",
    "        'MODEL': os.path.join('/home/david/SemesterProject/Models',MODEL),\n",
    "        'CKPT' : os.path.join('/home/david/SemesterProject/Models',MODEL,'CKPT'),\n",
    "        'TFLITE' : os.path.join('/home/david/SemesterProject/Models','TFLITE'),\n",
    "        'DATA' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half'),\n",
    "        'IMAGE' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half','images'),\n",
    "        'CLASS_ANN' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half','class_annotation'),\n",
    "        'INSTANCE_ANN' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half','instance_annotation'),\n",
    "        'COCO' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half','coco_data'),\n",
    "    }\n",
    "else:\n",
    "    paths = {\n",
    "        'MODEL': os.path.join('/home/david/SemesterProject/Models',MODEL),\n",
    "        'CKPT' : os.path.join('/home/david/SemesterProject/Models',MODEL,'CKPT'),\n",
    "        'TFLITE' : os.path.join('/home/david/SemesterProject/Models','TFLITE'),\n",
    "        'DATA' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half'),\n",
    "        'IMAGE' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half','images'),\n",
    "        'CLASS_ANN' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half','class_annotation'),\n",
    "        'INSTANCE_ANN' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half','instance_annotation'),\n",
    "        'COCO' : os.path.join('/home/david/BlenderProc',DATA,OUTPUT,'half','coco_data'),\n",
    "    }\n",
    "\n",
    "files = {\n",
    "    'OUTPUT_TFLITE_MODEL': os.path.join(paths['TFLITE'],MODEL + '.tflite'),\n",
    "    'OUTPUT_TFLITE_MODEL_METADATA': os.path.join(paths['TFLITE'],MODEL + '_metadata.tflite'),\n",
    "    'OUTPUT_TFLITE_LABEL_MAP': os.path.join(paths['TFLITE'],MODEL + '_tflite_label_map.txt'),\n",
    "    'CREATE_COCO_TF_RECORD': os.path.join(paths['MODEL'], 'research', 'object_detection', 'dataset_tools', 'create_coco_tf_record.py'),\n",
    "    'COCO_ANN': os.path.join(paths['COCO'], 'coco_annotations.json'),\n",
    "}\n",
    "\n",
    "for path in paths.values():\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Training Dataset contains 1 images.\n"
     ]
    }
   ],
   "source": [
    "image_count = len(glob(os.path.join(paths[\"IMAGE\"],\"*.png\")))\n",
    "print(\"The Training Dataset contains {IMAGES_SIZE} images.\".format(IMAGES_SIZE = image_count))\n",
    "\n",
    "\n",
    "BATCH_SIZE = 15 #reel: einiges gr√∂sser exp 32 aber schaue bei image segmentation nach\n",
    "\n",
    "IMG_SIZE_HEIGHT = 1280\n",
    "IMG_SIZE_WIDTH = 720\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(img_path):\n",
    "    \"\"\"Load an image and its annotation (mask) and returning\n",
    "    a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path : str\n",
    "        Image (not the mask) location.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary mapping an image and its annotation.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.io.decode_png(image, channels=3)\n",
    "\n",
    "    mask_path = tf.strings.regex_replace(img_path, \"images\", \"class_annotation\")\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    # The masks contain a class index for each pixels\n",
    "    mask = tf.io.decode_png(mask, channels=1)\n",
    "\n",
    "    return (image,mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[60 56 63 66 65 67 68 69 69 70]\n",
      " [50 66 67 67 67 69 68 68 68 68]\n",
      " [63 63 64 67 69 68 68 69 69 69]\n",
      " [65 66 66 67 68 67 67 67 67 67]\n",
      " [68 68 67 67 67 68 67 68 67 68]\n",
      " [68 68 67 68 68 68 68 69 68 69]\n",
      " [67 68 68 68 68 68 69 68 68 69]\n",
      " [67 67 68 68 68 68 68 69 69 69]\n",
      " [69 68 69 69 68 69 69 68 69 69]\n",
      " [70 70 69 69 69 68 68 69 69 69]], shape=(10, 10), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "dataset = tf.data.Dataset.list_files(os.path.join(paths[\"IMAGE\"],\"*.png\"))\n",
    "dataset = dataset.map(parse_image,num_parallel_calls=AUTOTUNE)\n",
    "dataset\n",
    "\n",
    "for im,lab in dataset:\n",
    "    print(im[0:10,0:10,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Dataset as TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "  if isinstance(value, type(tf.constant(0))):\n",
    "    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "  \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _image_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(\n",
    "        bytes_list=tf.train.BytesList(value=[tf.io.encode_jpeg(value).numpy()])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(f0,f1):\n",
    "  \"\"\"\n",
    "  Creates a tf.train.Example message ready to be written to a file.\n",
    "  \"\"\"\n",
    "  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "  # data type.\n",
    "  feature = {\n",
    "      'image': _image_feature(f0),\n",
    "      'mask': _image_feature(f1),\n",
    "  }\n",
    "\n",
    "  # Create a Features message using tf.train.Example.\n",
    "\n",
    "  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "  return example_proto.SerializeToString()\n",
    "\n",
    "def tf_serialize_example(f0,f1):\n",
    "  tf_string = tf.py_function(\n",
    "    serialize_example,\n",
    "    (f0,f1),  # Pass these args to the above function.\n",
    "    tf.string)      # The return type is `tf.string`.\n",
    "  return tf.reshape(tf_string, ()) # The result is a scalar.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.list_files(os.path.join(paths[\"IMAGE\"],\"*.png\"),seed = SEED)\n",
    "dataset = dataset.map(parse_image,num_parallel_calls=AUTOTUNE)\n",
    "serialized_dataset = dataset.map(tf_serialize_example)\n",
    "filename = 'test.tfrecord'\n",
    "writer = tf.data.experimental.TFRecordWriter(filename)\n",
    "writer.write(serialized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read TFRecords Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        \"mask\": tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    example[\"image\"] = tf.io.decode_jpeg(example[\"image\"], channels=3)\n",
    "    example[\"mask\"] = tf.io.decode_jpeg(example[\"mask\"], channels=1)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec={'image': TensorSpec(shape=(None, None, 3), dtype=tf.uint8, name=None), 'mask': TensorSpec(shape=(None, None, 1), dtype=tf.uint8, name=None)}>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = ['test.tfrecord']\n",
    "dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "dataset = dataset.map(parse_tfrecord_fn)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[60 56 63 66 65 67 68 69 69 70]\n",
      " [50 66 67 67 67 69 68 68 68 68]\n",
      " [63 63 64 67 69 68 68 69 69 69]\n",
      " [65 66 66 67 68 67 67 67 67 67]\n",
      " [68 68 67 67 67 68 67 68 67 68]\n",
      " [68 68 67 68 68 68 68 69 68 69]\n",
      " [67 68 68 68 68 68 69 68 68 69]\n",
      " [67 67 68 68 68 68 68 69 69 69]\n",
      " [69 68 69 69 68 69 69 68 69 69]\n",
      " [70 70 69 69 69 68 68 69 69 69]], shape=(10, 10), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "for im,lab in dataset:\n",
    "    print(im[0:10,0:10,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
